# Feature Specification: RAG Pipeline Setup & Embedding Storage

**Feature Branch**: `main`
**Created**: 2025-12-21
**Status**: Draft
**Input**: User description: "SPEC NAME: Spec 3: RAG Pipeline Setup & Embedding Storage CONTEXT: - Project: AI-native textbook + RAG chatbot system. - Phase 2 focuses on **retrieval-augmented generation (RAG) integration**. - Deployment URLs of frontend are available and will be used for pipeline testing. OBJECTIVE: - Build a **RAG pipeline** that extracts content from the textbook, creates embeddings, and stores them in a vector database. - Use **Cohere embedding model `embed-english-v3.0`** for generating embeddings. - Store vectors in **Qdrant** for retrieval. - Scaffold backend code to support later RAG retrieval by the agent. SCOPE: Includes: - Extracting and processing textbook content - Generating embeddings using Cohere `embed-english-v3.0` - Storing embeddings in Qdrant - Using **deployed frontend URLs** for data verification - Setting up **FastAPI backend structure** to host the pipeline ARCHITECTURE PRINCIPLES: - Backend code lives in `backend` folder - Use **uv package** project initialization - All Phase 2 pipeline code in **single modular file** (e.g., `main.py`) - Clean, maintainable, and scalable design for future agent integration"

## User Scenarios & Testing (mandatory)

### User Story 1 - Textbook Content Extraction & Embedding (Priority: P1)

As a system administrator, I need to extract all relevant content from the deployed Docusaurus textbook, generate embeddings for each meaningful chunk of text using Cohere `embed-english-v3.0`, and store these embeddings, along with their original text and metadata, into a Qdrant vector database. This will enable the RAG chatbot to retrieve relevant information from the textbook.

**Why this priority**: This is the foundational step for the entire RAG pipeline, making textbook content searchable and retrievable. Without it, the RAG chatbot cannot function.

**Independent Test**: Can be fully tested by running the RAG pipeline script against the deployed frontend URLs, verifying that the Qdrant database is populated with a sufficient number of embeddings, and that a sample query returns relevant text chunks.

**Acceptance Scenarios**:

1.  **Given** the Docusaurus frontend is deployed at a known URL, **When** the RAG pipeline is executed, **Then** it successfully extracts text content from all chapters of the textbook.
2.  **Given** extracted text content, **When** embeddings are generated using Cohere `embed-english-v3.0`, **Then** the Cohere API is successfully called, and vector embeddings are created.
3.  **Given** generated embeddings, **When** the Qdrant database is initialized and populated, **Then** all embeddings are successfully stored in Qdrant with associated metadata (e.g., source chapter, original text chunk).
4.  **Given** a populated Qdrant database, **When** a sample text query related to the textbook content is executed against Qdrant, **Then** it returns relevant text chunks from the textbook.

---

### Edge Cases

-   What happens when a deployed frontend URL is inaccessible? (System should log an error and attempt to proceed with other URLs or retry).
-   How does the system handle very large chapters or documents during embedding generation? (Should handle chunking appropriately to stay within model token limits).
-   What happens if the Cohere API rate limit is hit? (System should implement retry logic with exponential backoff).
-   What happens if Qdrant is unavailable during storage? (System should implement retry logic or report a critical error).

## Requirements (mandatory)

### Functional Requirements

-   **FR-001**: The system MUST extract content from the deployed Docusaurus frontend using its public URLs.
-   **FR-002**: The system MUST use the Cohere `embed-english-v3.0` model for generating text embeddings.
-   **FR-003**: The system MUST store the generated embeddings in a Qdrant vector database.
-   **FR-004**: The system MUST include the original text chunk and relevant metadata (e.g., chapter title, module name, page URL) alongside each embedding in Qdrant.
-   **FR-005**: The backend MUST be structured using FastAPI.
-   **FR-006**: All RAG pipeline code for this phase MUST reside in a single modular file (e.g., `main.py`) within the `backend` folder.
-   **FR-007**: The system MUST be initialized using the `uv package` for dependency management.
-   **FR-008**: The pipeline MUST be designed for clean, maintainable, and scalable integration with future agent retrieval logic.

### Key Entities

-   **Text Chunk**: A segment of text extracted from the textbook.
-   **Embedding**: A vector representation of a text chunk generated by Cohere `embed-english-v3.0`.
-   **Vector Database (Qdrant)**: Stores embeddings and associated metadata, enabling semantic search.
-   **Metadata**: Information associated with each text chunk, such as source chapter, module, URL.

## Success Criteria (mandatory)

### Measurable Outcomes

-   **SC-001**: 100% of the deployed textbook chapters are successfully extracted and processed by the RAG pipeline.
-   **SC-002**: All processed text chunks are successfully converted into embeddings using Cohere `embed-english-v3.0`.
-   **SC-003**: All generated embeddings are successfully stored in Qdrant, each with correct metadata.
-   **SC-004**: A sample query for a textbook topic returns the top 3 most relevant textbook chunks from Qdrant with an accuracy of >80% (judged manually).
